{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:11:54.333656Z",
     "start_time": "2024-06-12T21:11:54.330838Z"
    }
   },
   "source": [
    "# Following this guid to benchmark PyTorch operations: https://pytorch.org/tutorials/recipes/recipes/benchmark.html#benchmarking-with-torch-utils-benchmark-timer\n",
    "\n",
    "import torch\n",
    "import torch.utils.benchmark as benchmark\n"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:11:56.253655Z",
     "start_time": "2024-06-12T21:11:56.247676Z"
    }
   },
   "source": [
    "num_selections = 10\n",
    "\n",
    "# define the functions to compare/benchmark/time\n",
    "def index_using_gather(tensor, indices):\n",
    "    \"\"\"Selects elements from a tensor using gather (for N, 1).\"\"\"\n",
    "    return torch.gather(tensor, dim=0, index=indices.unsqueeze(1))  # Since dim is 1\n",
    "\n",
    "def index_using_integral_indexing(tensor, indices):\n",
    "    \"\"\"Selects elements from a tensor using integer indexing (for N, 1).\"\"\"\n",
    "    return tensor[indices]  # Direct indexing on the first dimension\n",
    "\n",
    "def index_using_index_select(tensor, indices):\n",
    "    return torch.index_select(tensor, 0, indices)\n",
    "\n",
    "# Sample tensor and indices\n",
    "tensor = torch.randn(1000, 1)\n",
    "indices = torch.randint(0, tensor.shape[0], (num_selections, ))  # Generate random indices for N\n"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:11:58.788683Z",
     "start_time": "2024-06-12T21:11:58.774244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tensor = tensor.to(\"cuda\")\n",
    "indices = indices.to(\"cuda\")"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[67], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m tensor \u001B[38;5;241m=\u001B[39m \u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m indices \u001B[38;5;241m=\u001B[39m indices\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:12:01.456047Z",
     "start_time": "2024-06-12T21:12:01.373567Z"
    }
   },
   "source": [
    "# Benchmarking with pytorch.utils.benchmark\n",
    "t_gather = benchmark.Timer(\n",
    "    stmt=\"index_using_gather(tensor.clone(), indices.clone())\",\n",
    "    setup=\"from __main__ import index_using_gather, tensor, indices\",\n",
    ")\n",
    "t_indexing = benchmark.Timer(\n",
    "    stmt=\"index_using_integral_indexing(tensor.clone(), indices.clone())\",\n",
    "    setup=\"from __main__ import index_using_integral_indexing, tensor, indices\",\n",
    ")\n",
    "t_index_select = benchmark.Timer(\n",
    "    stmt=\"index_using_index_select(tensor.clone(), indices.clone())\",\n",
    "    setup=\"from __main__ import index_using_index_select, tensor, indices\",\n",
    ")\n",
    "\n",
    "# Repeatedly run the timers for more accurate measurements\n",
    "print(\"Gather:\")\n",
    "print(t_gather.timeit(number=400000))  # Run many times for better accuracy\n",
    "print(\"Integer Indexing:\")\n",
    "print(t_indexing.timeit(number=400000))\n",
    "print(\"Index Select:\")\n",
    "print(t_index_select.timeit(number=400000))\n",
    "\n",
    "# Ensure outputs are the same\n",
    "assert torch.allclose(index_using_gather(tensor.clone(), indices.clone()), index_using_integral_indexing(tensor.clone(), indices.clone()))\n",
    "assert torch.allclose(index_using_gather(tensor.clone(), indices.clone()), index_using_index_select(tensor.clone(), indices.clone()))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gather:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[68], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Repeatedly run the timers for more accurate measurements\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGather:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mt_gather\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnumber\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m400000\u001B[39;49m\u001B[43m)\u001B[49m)  \u001B[38;5;66;03m# Run many times for better accuracy\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInteger Indexing:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(t_indexing\u001B[38;5;241m.\u001B[39mtimeit(number\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m400000\u001B[39m))\n",
      "File \u001B[0;32m~/miniforge3/envs/test/lib/python3.10/site-packages/torch/utils/benchmark/utils/timer.py:274\u001B[0m, in \u001B[0;36mTimer.timeit\u001B[0;34m(self, number)\u001B[0m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Mirrors the semantics of timeit.Timer.timeit().\u001B[39;00m\n\u001B[1;32m    268\u001B[0m \n\u001B[1;32m    269\u001B[0m \u001B[38;5;124;03mExecute the main statement (`stmt`) `number` times.\u001B[39;00m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;124;03mhttps://docs.python.org/3/library/timeit.html#timeit.Timer.timeit\u001B[39;00m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m common\u001B[38;5;241m.\u001B[39mset_torch_threads(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_spec\u001B[38;5;241m.\u001B[39mnum_threads):\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;66;03m# Warmup\u001B[39;00m\n\u001B[0;32m--> 274\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_timeit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnumber\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnumber\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m common\u001B[38;5;241m.\u001B[39mMeasurement(\n\u001B[1;32m    277\u001B[0m         number_per_run\u001B[38;5;241m=\u001B[39mnumber,\n\u001B[1;32m    278\u001B[0m         raw_times\u001B[38;5;241m=\u001B[39m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeit(number\u001B[38;5;241m=\u001B[39mnumber)],\n\u001B[1;32m    279\u001B[0m         task_spec\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_spec\n\u001B[1;32m    280\u001B[0m     )\n",
      "File \u001B[0;32m~/miniforge3/envs/test/lib/python3.10/site-packages/torch/utils/benchmark/utils/timer.py:264\u001B[0m, in \u001B[0;36mTimer._timeit\u001B[0;34m(self, number)\u001B[0m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_timeit\u001B[39m(\u001B[38;5;28mself\u001B[39m, number: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;66;03m# Even calling a timer in C++ takes ~50 ns, so no real operation should\u001B[39;00m\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;66;03m# take less than 1 ns. (And this prevents divide by zero errors.)\u001B[39;00m\n\u001B[0;32m--> 264\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_timer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnumber\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;241m1e-9\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/test/lib/python3.10/timeit.py:178\u001B[0m, in \u001B[0;36mTimer.timeit\u001B[0;34m(self, number)\u001B[0m\n\u001B[1;32m    176\u001B[0m gc\u001B[38;5;241m.\u001B[39mdisable()\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 178\u001B[0m     timing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gcold:\n",
      "File \u001B[0;32m<timeit-src>:4\u001B[0m, in \u001B[0;36minner\u001B[0;34m(_it, _timer)\u001B[0m\n",
      "File \u001B[0;32m~/miniforge3/envs/test/lib/python3.10/site-packages/torch/utils/benchmark/utils/timer.py:18\u001B[0m, in \u001B[0;36mtimer\u001B[0;34m()\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtimer\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[0;32m---> 18\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msynchronize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m timeit\u001B[38;5;241m.\u001B[39mdefault_timer()\n",
      "File \u001B[0;32m~/miniforge3/envs/test/lib/python3.10/site-packages/torch/cuda/__init__.py:792\u001B[0m, in \u001B[0;36msynchronize\u001B[0;34m(device)\u001B[0m\n\u001B[1;32m    790\u001B[0m _lazy_init()\n\u001B[1;32m    791\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdevice(device):\n\u001B[0;32m--> 792\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cuda_synchronize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:50:15.263575Z",
     "start_time": "2024-06-12T19:50:15.259846Z"
    }
   },
   "cell_type": "code",
   "source": "tensor.device",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelforge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
