from typing_extensions import Annotated
from pydantic import BeforeValidator, PlainSerializer

import numpy as np
from typing import Union

__all__ = ["NdArray"]


# Define a serializer for numpy ndarrays for pydantic
def nd_array_validator(v):
    return v


def nd_array_serializer(v):
    return str(v)


NdArray = Annotated[
    np.ndarray,
    BeforeValidator(nd_array_validator),
    PlainSerializer(nd_array_serializer, return_type=str),
]


def _convert_list_to_ndarray(value: Union[list, np.ndarray]):
    """
    This will convert a list to a numpy ndarray

    If the input is a numpy ndarray, nothing will be changed.

    Parameters
    ----------
    value: Union[list, np.ndarray]
        The value to convert to a numpy ndarray

    Returns
    -------
        np.ndarray

    """
    if isinstance(value, list):
        return np.array(value)
    return value


def gzip_file(
    input_file_name: str,
    input_file_dir: str,
    keep_original: bool = False,
) -> None:
    """
    Gzip a file. Note, this will overwrite an existing gzipped file with the same name.

    This will output a gzipped file with the same name as the input file, but with a .gz extension,
    to the same directory as the input file. The original file will be deleted unless keep_original is set to True.

    Note, this uses the command line gzip with level 9 compression rather than the python library.
    This is done to ensure the same metadata is encoded and thus the md5 checksum for a gzipped file
    generated by this function will be the same as the one generated via the command line.

    Parameters
    ----------
    input_file_name : str
        The name of the file to gzip.
    input_file_dir : str
        The directory containing the file to gzip.
    keep_original : bool, optional
        If True, the original file will be kept. If False, only the compressed file will be kept.
        Default is False.


    Returns
    -------
        Tuple[int, str]
            The size (byte) of the gzipped file and the name of the gzipped file.

    """
    import os

    gzip_file_name = f"{input_file_name}.gz"

    # make sure we can handle a tilda in the path
    input_file_dir = os.path.expanduser(input_file_dir)

    # use system gzip rather than python library
    if keep_original:
        os.system(f"gzip -9 -k -f {input_file_dir}/{input_file_name}")

    else:
        os.system(f"gzip -9 -f {input_file_dir}/{input_file_name}")

    print(f"{input_file_dir}/{gzip_file_name}")
    return (
        os.path.getsize(f"{input_file_dir}/{gzip_file_name}"),
        gzip_file_name,
    )


from dataclasses import dataclass


class YamlMetaData:

    def __init__(
        self,
        version_name: str,
        about: str,
        hdf5_file_name: str,
        hdf5_file_dir: str,
        hdf5_checksum: str,
        available_properties: list,
    ):

        self.version_name = version_name
        self.about = about
        self.hdf5_file_name = hdf5_file_name
        self.hdf5_file_dir = hdf5_file_dir
        self.hdf5_checksum = hdf5_checksum
        self.available_properties = available_properties

    def compress_hdf5(self):
        """
        Compress the hdf5 file using gzip
        """
        from modelforge.curate.utils import gzip_file

        length, filename = gzip_file(
            input_file_name=self.hdf5_file_name,
            input_file_dir=self.hdf5_file_dir,
            keep_original=False,
        )

        self.gzipped_file_name = filename
        self.gzipped_length = length

        from modelforge.utils.remote import get_md5_checksum

        self.gzipped_checksum = get_md5_checksum(
            file_name=self.gzipped_file_name, file_path=self.hdf5_file_dir
        )

    def to_dict(self):

        data = {}
        data[self.version_name] = {
            "hdf5_schema": 2,
            "available_properties": self.available_properties,
            "about": self.about,
            "remote_dataset": {
                "doi": " ",
                "url": " ",
                "gz_data_file": {
                    "length": self.gzipped_length,
                    "md5": self.gzipped_checksum,
                    "file_name": self.gzipped_file_name,
                },
                "hdf5_data_file": {
                    "md5": self.hdf5_checksum,
                    "file_name": self.hdf5_file_name,
                },
            },
        }
