from typing_extensions import Annotated
from pydantic import BeforeValidator, PlainSerializer

import numpy as np
from typing import Union

__all__ = ["NdArray"]


# Define a serializer for numpy ndarrays for pydantic
def nd_array_validator(v):
    return v


def nd_array_serializer(v):
    return str(v)


NdArray = Annotated[
    np.ndarray,
    BeforeValidator(nd_array_validator),
    PlainSerializer(nd_array_serializer, return_type=str),
]


def _convert_list_to_ndarray(value: Union[list, np.ndarray]):
    """
    This will convert a list to a numpy ndarray

    If the input is a numpy ndarray, nothing will be changed.

    Parameters
    ----------
    value: Union[list, np.ndarray]
        The value to convert to a numpy ndarray

    Returns
    -------
        np.ndarray

    """
    if isinstance(value, list):
        return np.array(value)
    return value


def gzip_file(
    input_file_name: str,
    input_file_dir: str,
    keep_original: bool = False,
) -> None:
    """
    Gzip a file. Note, this will overwrite an existing gzipped file with the same name.

    This will output a gzipped file with the same name as the input file, but with a .gz extension,
    to the same directory as the input file. The original file will be deleted unless keep_original is set to True.

    Note, this uses the command line gzip with level 9 compression rather than the python library.
    This is done to ensure the same metadata is encoded and thus the md5 checksum for a gzipped file
    generated by this function will be the same as the one generated via the command line.

    Parameters
    ----------
    input_file_name : str
        The name of the file to gzip.
    input_file_dir : str
        The directory containing the file to gzip.
    keep_original : bool, optional
        If True, the original file will be kept. If False, only the compressed file will be kept.
        Default is False.


    Returns
    -------
        Tuple[int, str]
            The size (byte) of the gzipped file and the name of the gzipped file.

    """
    import os

    gzip_file_name = f"{input_file_name}.gz"

    # make sure we can handle a tilda in the path
    input_file_dir = os.path.expanduser(input_file_dir)

    # use system gzip rather than python library
    if keep_original:
        os.system(f"gzip -9 -k -f {input_file_dir}/{input_file_name}")

    else:
        os.system(f"gzip -9 -f {input_file_dir}/{input_file_name}")

    return (
        os.path.getsize(f"{input_file_dir}/{gzip_file_name}"),
        gzip_file_name,
    )


from dataclasses import dataclass


class VersionMetadata:
    """
    Class to hold the metadata for a version of a dataset.

    This will provide functionality to write the metadata to the format used in the .yaml files.

    this will also provide a function to compress the hdf5 file, extracting the checksum and file length
    needed for the yaml file.

    the DOI and URL files will be left blank, as those come after the dataset is uploaded.

    Parameters
    ----------
    version_name : str
        The name of the version of the dataset.
    about : str
        A description of the dataset.
    hdf5_file_name : str
        The name of the hdf5 file.
    hdf5_file_dir : str
        The directory containing the hdf5 file.
    available_properties : list
        A list of the available properties in the dataset.
    remote_dataset : bool, optional
        If True, the dataset is a remote dataset (i.e., stored on zenodo). If False, the dataset is a local dataset.
        A local dataset will not be compressed and there is slightly different metadata written in the yaml file
        Default is True.
    """

    def __init__(
        self,
        version_name: str,
        about: str,
        hdf5_file_name: str,
        hdf5_file_dir: str,
        available_properties: list,
        remote_dataset: bool = True,
    ):

        self.version_name = version_name
        self.about = about
        self.hdf5_file_name = hdf5_file_name
        self.hdf5_file_dir = hdf5_file_dir
        self.available_properties = available_properties
        self.remote_dataset = remote_dataset

        from modelforge.utils.remote import calculate_md5_checksum

        self.hdf5_checksum = calculate_md5_checksum(
            file_name=self.hdf5_file_name, file_path=self.hdf5_file_dir
        )

    def _compress_hdf5(self):
        """
        Compress the hdf5 file using gzip
        """
        from modelforge.curate.utils import gzip_file

        length, filename = gzip_file(
            input_file_name=self.hdf5_file_name,
            input_file_dir=self.hdf5_file_dir,
            keep_original=True,
        )

        self.gzipped_file_name = filename
        self.gzipped_length = length

        from modelforge.utils.remote import calculate_md5_checksum

        self.gzipped_checksum = calculate_md5_checksum(
            file_name=self.gzipped_file_name, file_path=self.hdf5_file_dir
        )

    def _remote_dataset_to_dict(self):
        import re

        data = {}
        data[self.version_name] = {
            "hdf5_schema": 2,
            "available_properties": self.available_properties,
            "about": re.sub(" +", " ", self.about.replace("\n", " ")),
            "remote_dataset": {
                "doi": " ",
                "url": " ",
                "gz_data_file": {
                    "length": self.gzipped_length,
                    "md5": self.gzipped_checksum,
                    "file_name": self.gzipped_file_name,
                },
                "hdf5_data_file": {
                    "md5": self.hdf5_checksum,
                    "file_name": self.hdf5_file_name,
                },
            },
        }

        return data

    def _local_dataset_to_dict(self):
        import re

        data = {}
        data[self.version_name] = {
            "hdf5_schema": 2,
            "available_properties": self.available_properties,
            "about": re.sub(" +", " ", self.about.replace("\n", " ")),
            "local_dataset": {
                "hdf5_data_file": {
                    "md5": self.hdf5_checksum,
                    "file_name": f"{self.hdf5_file_dir}/{self.hdf5_file_name}",
                },
            },
        }
        return data

    def to_yaml(self, file_name: str, file_path: str):
        """
        Compress the hdf5 file and write the metadata to a yaml file.

        Parameters
        ----------
        file_name : str
            The name of the file to write to.
        file_path : str
            The path to the file to write to.

        Returns
        -------
            None

        """
        import os
        import yaml
        import time

        # make sure we can handle a tilda in the path
        file_path = os.path.expanduser(file_path)
        yaml_file = f"{file_path}/{file_name}"

        # first write out a header for the file so we know when it was generated
        with open(yaml_file, "w") as f:
            f.write(
                f"# Processed on {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}.\n"
            )
        if self.remote_dataset:
            self._compress_hdf5()
            with open(f"{file_path}/{file_name}", "w") as f:
                yaml.dump(self._remote_dataset_to_dict(), f)
        else:
            with open(f"{file_path}/{file_name}", "w") as f:
                yaml.dump(self._local_dataset_to_dict(), f)
